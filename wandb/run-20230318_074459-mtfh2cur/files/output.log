<parse_config.ConfigParser object at 0x14eebd4aedd0>
loading features >>> [Total: 0.3s] (gnode030:data/features/BERT/bert-large-cased/clip_name/agg/agg.npy)
>>> Currently using 2.0% of system memory 1.6 GB/132.4 GB
loading features >>> [Total: 8.0s] (gnode030:data/features/BERT/bert-large-cased/description/agg/agg_word.npy)
>>> Currently using 5.2% of system memory 6.0 GB/128.0 GB
loading features >>> [Total: 0.2s] (gnode030:data/features/SE-ResNet-50/256D_vgg_face2/agg/agg_feats_mean.npy)
>>> Currently using 5.2% of system memory 6.0 GB/128.0 GB
loading features >>> [Total: 0.4s] (gnode030:data/features/SE-ResNet-154/pred_imagenet_25fps_256px_stride1_offset0/agg/agg_feats_mean.npy)
>>> Currently using 5.3% of system memory 6.0 GB/128.0 GB
loading features >>> [Total: 0.9s] (gnode030:data/features/DenseNet-161/pred_scene_25fps_256px_stride1_offset0/agg/agg_feats_mean.npy)
>>> Currently using 5.4% of system memory 6.2 GB/127.8 GB
loading features >>> [Total: 5.0s] (gnode030:data/features/BERT/bert-large-cased/speech/agg/agg.npy)
>>> Currently using 6.8% of system memory 8.1 GB/125.9 GB
loading features >>> [Total: 0.3s] (gnode030:data/features/I3D/pred_i3d_25fps_256px_stride25_offset0_inner_stride1/agg/agg_feats_mean.npy)
>>> Currently using 6.8% of system memory 8.1 GB/125.8 GB
loading features >>> [Total: 0.3s] (gnode030:data/features/S3DG/pred_s3dg_10fps_256px_stride16_offset0_inner_stride1/agg/agg_feats_mean.npy)
>>> Currently using 6.9% of system memory 8.2 GB/125.8 GB
train size: 24098 clips
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
>>> Currently using 6.9% of system memory 8.2 GB/125.7 GB
val size: 3360 clips
MoEE(
  (aggregation): ModuleDict(
    (label): NetVLAD(
      (batch_norm): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (face): MeanToken()
    (speech): MeanToken()
  )
  (video_GU): ModuleDict(
    (clip_name): Gated_Embedding_Unit(
      (fc): Linear(in_features=1024, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (face): Gated_Embedding_Unit(
      (fc): Linear(in_features=256, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (rgb): Gated_Embedding_Unit(
      (fc): Linear(in_features=2048, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (scene): Gated_Embedding_Unit(
      (fc): Linear(in_features=2208, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (speech): Gated_Embedding_Unit(
      (fc): Linear(in_features=1024, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (video): Gated_Embedding_Unit(
      (fc): Linear(in_features=1024, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (s3d): Gated_Embedding_Unit(
      (fc): Linear(in_features=1024, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (clip_GU): ModuleList(
    (0): Identity()
  )
  (text_GU): ModuleDict(
    (clip_name): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (description): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (face): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (rgb): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (scene): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (speech): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (video): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (s3d): Gated_Embedding_Unit(
      (fc): Linear(in_features=10240, out_features=512, bias=True)
      (cg): Context_Gating(
        (fc): Linear(in_features=512, out_features=512, bias=True)
        (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (text_clip): ModuleList(
    (0): Identity()
  )
  (moe_fc): Linear(in_features=10240, out_features=7, bias=True)
)
Trainable parameters: 50398249
Train Epoch: 1 [0/24098 (0%)] Loss: 0.121640
Train Epoch: 1 [512/24098 (2%)] Loss: 0.121449
Train Epoch: 1 [1024/24098 (4%)] Loss: 0.121020
Train Epoch: 1 [1536/24098 (6%)] Loss: 0.121309
Train Epoch: 1 [2048/24098 (8%)] Loss: 0.121203
Train Epoch: 1 [2560/24098 (11%)] Loss: 0.121414
Train Epoch: 1 [3072/24098 (13%)] Loss: 0.121358
Train Epoch: 1 [3584/24098 (15%)] Loss: 0.120882
Train Epoch: 1 [4096/24098 (17%)] Loss: 0.120839
Train Epoch: 1 [4608/24098 (19%)] Loss: 0.121091
Train Epoch: 1 [5120/24098 (21%)] Loss: 0.121453
Train Epoch: 1 [5632/24098 (23%)] Loss: 0.121327
Train Epoch: 1 [6144/24098 (25%)] Loss: 0.121328
Train Epoch: 1 [6656/24098 (28%)] Loss: 0.121232
Train Epoch: 1 [7168/24098 (30%)] Loss: 0.121546
Train Epoch: 1 [7680/24098 (32%)] Loss: 0.121030
Train Epoch: 1 [8192/24098 (34%)] Loss: 0.121141
Train Epoch: 1 [8704/24098 (36%)] Loss: 0.121527
Train Epoch: 1 [9216/24098 (38%)] Loss: 0.121110
Train Epoch: 1 [9728/24098 (40%)] Loss: 0.121080
Train Epoch: 1 [10240/24098 (42%)] Loss: 0.120885
Train Epoch: 1 [10752/24098 (45%)] Loss: 0.121357
Train Epoch: 1 [11264/24098 (47%)] Loss: 0.121467
Train Epoch: 1 [11776/24098 (49%)] Loss: 0.121530
Train Epoch: 1 [12288/24098 (51%)] Loss: 0.121538
Train Epoch: 1 [12800/24098 (53%)] Loss: 0.121338
Train Epoch: 1 [13312/24098 (55%)] Loss: 0.121457
Train Epoch: 1 [13824/24098 (57%)] Loss: 0.120951
Train Epoch: 1 [14336/24098 (59%)] Loss: 0.121203
Train Epoch: 1 [14848/24098 (62%)] Loss: 0.121030
Train Epoch: 1 [15360/24098 (64%)] Loss: 0.121715
Train Epoch: 1 [15872/24098 (66%)] Loss: 0.121240
Train Epoch: 1 [16384/24098 (68%)] Loss: 0.121371
Train Epoch: 1 [16896/24098 (70%)] Loss: 0.121028
Train Epoch: 1 [17408/24098 (72%)] Loss: 0.121347
Train Epoch: 1 [17920/24098 (74%)] Loss: 0.121316
Train Epoch: 1 [18432/24098 (76%)] Loss: 0.121758
Train Epoch: 1 [18944/24098 (79%)] Loss: 0.121259
Train Epoch: 1 [19456/24098 (81%)] Loss: 0.121393
Train Epoch: 1 [19968/24098 (83%)] Loss: 0.121170
Train Epoch: 1 [20480/24098 (85%)] Loss: 0.120921
Train Epoch: 1 [20992/24098 (87%)] Loss: 0.121108
Train Epoch: 1 [21504/24098 (89%)] Loss: 0.121280
Train Epoch: 1 [22016/24098 (91%)] Loss: 0.121454
Train Epoch: 1 [22528/24098 (93%)] Loss: 0.120839
Train Epoch: 1 [23040/24098 (96%)] Loss: 0.121165
Train Epoch: 1 [23552/24098 (98%)] Loss: 0.121349
Train Epoch: 1 [24064/24098 (100%)] Loss: 0.121353
[t2v_metrics]MovieClips epoch 1, R@1: 0.0, R@5: 0.1, R@10 0.2, R@50 1.5MedR: 1654, MeanR: 1668.0
/home2/debashish.roy/miniconda3/envs/IS/lib/python3.7/site-packages/scipy/stats/stats.py:275: RuntimeWarning: divide by zero encountered in log
  log_a = np.log(np.array(a, dtype=dtype))
/home2/debashish.roy/miniconda3/envs/IS/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
[v2t_metrics]MovieClips epoch 1, R@1: 0.0, R@5: 0.1, R@10 0.2, R@50 1.3MedR: 1650, MeanR: 1650.3
    epoch          : 1
    loss           : 0.1212113482447771
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.12118886411190033
    val_t2v_metrics_R1: 0.02976190476190476
    val_t2v_metrics_R5: 0.11904761904761904
    val_t2v_metrics_R10: 0.23809523809523808
    val_t2v_metrics_R50: 1.5476190476190477
    val_t2v_metrics_MedR: 1654.0
    val_t2v_metrics_MeanR: 1668.0049107142856
    val_t2v_metrics_geometric_mean_R1-R5-R10: 0.09448815785524996
    val_v2t_metrics_R1: 0.0
    val_v2t_metrics_R5: 0.11904761904761904
    val_v2t_metrics_R10: 0.20833333333333334
    val_v2t_metrics_R50: 1.2797619047619047
    val_v2t_metrics_MedR: 1650.0
    val_v2t_metrics_MeanR: 1650.2943452380953
    val_v2t_metrics_geometric_mean_R1-R5-R10: 0.0
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch1.pth ...
Saving current best: model_best.pth ...
/ssd_scratch/cvit/debashish/CondensedMovies/trainer/trainer.py:136: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.
  with autograd.detect_anomaly():
Train Epoch: 2 [0/24098 (0%)] Loss: 0.121423
Train Epoch: 2 [512/24098 (2%)] Loss: 0.091731
Train Epoch: 2 [1024/24098 (4%)] Loss: 0.060849
Train Epoch: 2 [1536/24098 (6%)] Loss: 0.047189
Train Epoch: 2 [2048/24098 (8%)] Loss: 0.040101
Train Epoch: 2 [2560/24098 (11%)] Loss: 0.035037
Train Epoch: 2 [3072/24098 (13%)] Loss: 0.030055
Train Epoch: 2 [3584/24098 (15%)] Loss: 0.024911
Train Epoch: 2 [4096/24098 (17%)] Loss: 0.030866
Train Epoch: 2 [4608/24098 (19%)] Loss: 0.029370
Train Epoch: 2 [5120/24098 (21%)] Loss: 0.026580
Train Epoch: 2 [5632/24098 (23%)] Loss: 0.022839
Train Epoch: 2 [6144/24098 (25%)] Loss: 0.030522
Train Epoch: 2 [6656/24098 (28%)] Loss: 0.025127
Train Epoch: 2 [7168/24098 (30%)] Loss: 0.023662
Train Epoch: 2 [7680/24098 (32%)] Loss: 0.020192
Train Epoch: 2 [8192/24098 (34%)] Loss: 0.024339
Train Epoch: 2 [8704/24098 (36%)] Loss: 0.016193
Train Epoch: 2 [9216/24098 (38%)] Loss: 0.022490
Train Epoch: 2 [9728/24098 (40%)] Loss: 0.021793
Train Epoch: 2 [10240/24098 (42%)] Loss: 0.021453
Train Epoch: 2 [10752/24098 (45%)] Loss: 0.022602
Train Epoch: 2 [11264/24098 (47%)] Loss: 0.020941
Train Epoch: 2 [11776/24098 (49%)] Loss: 0.021507
Train Epoch: 2 [12288/24098 (51%)] Loss: 0.016275
Train Epoch: 2 [12800/24098 (53%)] Loss: 0.020269
Train Epoch: 2 [13312/24098 (55%)] Loss: 0.014820
Train Epoch: 2 [13824/24098 (57%)] Loss: 0.020294
Train Epoch: 2 [14336/24098 (59%)] Loss: 0.015271
Train Epoch: 2 [14848/24098 (62%)] Loss: 0.017529
Train Epoch: 2 [15360/24098 (64%)] Loss: 0.016571
Train Epoch: 2 [15872/24098 (66%)] Loss: 0.017249
Train Epoch: 2 [16384/24098 (68%)] Loss: 0.013235
Train Epoch: 2 [16896/24098 (70%)] Loss: 0.021065
Train Epoch: 2 [17408/24098 (72%)] Loss: 0.014944
Train Epoch: 2 [17920/24098 (74%)] Loss: 0.012209
Train Epoch: 2 [18432/24098 (76%)] Loss: 0.020965
Train Epoch: 2 [18944/24098 (79%)] Loss: 0.012626
Train Epoch: 2 [19456/24098 (81%)] Loss: 0.013612
Train Epoch: 2 [19968/24098 (83%)] Loss: 0.013227
Train Epoch: 2 [20480/24098 (85%)] Loss: 0.011647
Train Epoch: 2 [20992/24098 (87%)] Loss: 0.016675
Train Epoch: 2 [21504/24098 (89%)] Loss: 0.015880
Train Epoch: 2 [22016/24098 (91%)] Loss: 0.010186
Train Epoch: 2 [22528/24098 (93%)] Loss: 0.012253
Train Epoch: 2 [23040/24098 (96%)] Loss: 0.017954
Train Epoch: 2 [23552/24098 (98%)] Loss: 0.011306
Train Epoch: 2 [24064/24098 (100%)] Loss: 0.017313
[t2v_metrics]MovieClips epoch 2, R@1: 3.4, R@5: 15.0, R@10 22.8, R@50 48.5MedR: 55, MeanR: 170.3
[v2t_metrics]MovieClips epoch 2, R@1: 6.6, R@5: 18.9, R@10 27.6, R@50 54.1MedR: 40, MeanR: 149.4
    epoch          : 2
    loss           : 0.023651217046225893
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.01650250516831875
    val_t2v_metrics_R1: 3.392857142857143
    val_t2v_metrics_R5: 14.970238095238095
    val_t2v_metrics_R10: 22.827380952380953
    val_t2v_metrics_R50: 48.482142857142854
    val_t2v_metrics_MedR: 55.0
    val_t2v_metrics_MeanR: 170.3452380952381
    val_t2v_metrics_geometric_mean_R1-R5-R10: 10.505501506884917
    val_v2t_metrics_R1: 6.607142857142857
    val_v2t_metrics_R5: 18.928571428571427
    val_v2t_metrics_R10: 27.61904761904762
    val_v2t_metrics_R50: 54.13690476190476
    val_v2t_metrics_MedR: 40.0
    val_v2t_metrics_MeanR: 149.3982142857143
    val_v2t_metrics_geometric_mean_R1-R5-R10: 15.116343241041617
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch2.pth ...
Saving current best: model_best.pth ...
Train Epoch: 3 [0/24098 (0%)] Loss: 0.008191
Train Epoch: 3 [512/24098 (2%)] Loss: 0.005852
Train Epoch: 3 [1024/24098 (4%)] Loss: 0.006111
Train Epoch: 3 [1536/24098 (6%)] Loss: 0.007121
Train Epoch: 3 [2048/24098 (8%)] Loss: 0.004455
Train Epoch: 3 [2560/24098 (11%)] Loss: 0.003506
Train Epoch: 3 [3072/24098 (13%)] Loss: 0.006796
Train Epoch: 3 [3584/24098 (15%)] Loss: 0.008290
Train Epoch: 3 [4096/24098 (17%)] Loss: 0.004588
Train Epoch: 3 [4608/24098 (19%)] Loss: 0.003487
Train Epoch: 3 [5120/24098 (21%)] Loss: 0.005981
Train Epoch: 3 [5632/24098 (23%)] Loss: 0.007989
Train Epoch: 3 [6144/24098 (25%)] Loss: 0.003540
Train Epoch: 3 [6656/24098 (28%)] Loss: 0.005668
Train Epoch: 3 [7168/24098 (30%)] Loss: 0.007625
Train Epoch: 3 [7680/24098 (32%)] Loss: 0.003910
Train Epoch: 3 [8192/24098 (34%)] Loss: 0.004451
Train Epoch: 3 [8704/24098 (36%)] Loss: 0.005611
Train Epoch: 3 [9216/24098 (38%)] Loss: 0.006726
Train Epoch: 3 [9728/24098 (40%)] Loss: 0.005159
Train Epoch: 3 [10240/24098 (42%)] Loss: 0.005090
Train Epoch: 3 [10752/24098 (45%)] Loss: 0.005043
Train Epoch: 3 [11264/24098 (47%)] Loss: 0.006090
Train Epoch: 3 [11776/24098 (49%)] Loss: 0.004804
Train Epoch: 3 [12288/24098 (51%)] Loss: 0.006383
Train Epoch: 3 [12800/24098 (53%)] Loss: 0.004563
Train Epoch: 3 [13312/24098 (55%)] Loss: 0.003880
Train Epoch: 3 [13824/24098 (57%)] Loss: 0.004750
Train Epoch: 3 [14336/24098 (59%)] Loss: 0.004797
Train Epoch: 3 [14848/24098 (62%)] Loss: 0.003979
Train Epoch: 3 [15360/24098 (64%)] Loss: 0.004326
Train Epoch: 3 [15872/24098 (66%)] Loss: 0.004700
Train Epoch: 3 [16384/24098 (68%)] Loss: 0.003374
Train Epoch: 3 [16896/24098 (70%)] Loss: 0.004296
Train Epoch: 3 [17408/24098 (72%)] Loss: 0.005604
Train Epoch: 3 [17920/24098 (74%)] Loss: 0.005086
Train Epoch: 3 [18432/24098 (76%)] Loss: 0.004519
Train Epoch: 3 [18944/24098 (79%)] Loss: 0.005927
Train Epoch: 3 [19456/24098 (81%)] Loss: 0.006540
Train Epoch: 3 [19968/24098 (83%)] Loss: 0.004035
Train Epoch: 3 [20480/24098 (85%)] Loss: 0.005864
Train Epoch: 3 [20992/24098 (87%)] Loss: 0.003650
Train Epoch: 3 [21504/24098 (89%)] Loss: 0.004809
Train Epoch: 3 [22016/24098 (91%)] Loss: 0.004341
Train Epoch: 3 [22528/24098 (93%)] Loss: 0.005732
Train Epoch: 3 [23040/24098 (96%)] Loss: 0.003839
Train Epoch: 3 [23552/24098 (98%)] Loss: 0.003170
Train Epoch: 3 [24064/24098 (100%)] Loss: 0.004918
[t2v_metrics]MovieClips epoch 3, R@1: 6.8, R@5: 20.8, R@10 30.1, R@50 56.9MedR: 35, MeanR: 139.5
[v2t_metrics]MovieClips epoch 3, R@1: 10.0, R@5: 25.8, R@10 35.4, R@50 61.1MedR: 27, MeanR: 121.1
    epoch          : 3
    loss           : 0.005083685241817479
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.01405204739421606
    val_t2v_metrics_R1: 6.755952380952381
    val_t2v_metrics_R5: 20.773809523809526
    val_t2v_metrics_R10: 30.11904761904762
    val_t2v_metrics_R50: 56.904761904761905
    val_t2v_metrics_MedR: 35.0
    val_t2v_metrics_MeanR: 139.53095238095239
    val_t2v_metrics_geometric_mean_R1-R5-R10: 16.16893145927285
    val_v2t_metrics_R1: 10.0
    val_v2t_metrics_R5: 25.773809523809526
    val_v2t_metrics_R10: 35.357142857142854
    val_v2t_metrics_R50: 61.101190476190474
    val_v2t_metrics_MedR: 27.0
    val_v2t_metrics_MeanR: 121.08422619047619
    val_v2t_metrics_geometric_mean_R1-R5-R10: 20.88744211229713
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch3.pth ...
Saving current best: model_best.pth ...
Train Epoch: 4 [0/24098 (0%)] Loss: 0.002173
Train Epoch: 4 [512/24098 (2%)] Loss: 0.001513
Train Epoch: 4 [1024/24098 (4%)] Loss: 0.001764
Train Epoch: 4 [1536/24098 (6%)] Loss: 0.001546
Train Epoch: 4 [2048/24098 (8%)] Loss: 0.001686
Train Epoch: 4 [2560/24098 (11%)] Loss: 0.001412
Train Epoch: 4 [3072/24098 (13%)] Loss: 0.001698
Train Epoch: 4 [3584/24098 (15%)] Loss: 0.001673
Train Epoch: 4 [4096/24098 (17%)] Loss: 0.001074
Train Epoch: 4 [4608/24098 (19%)] Loss: 0.001845
Train Epoch: 4 [5120/24098 (21%)] Loss: 0.001440
Train Epoch: 4 [5632/24098 (23%)] Loss: 0.001825
Train Epoch: 4 [6144/24098 (25%)] Loss: 0.001572
Train Epoch: 4 [6656/24098 (28%)] Loss: 0.001136
Train Epoch: 4 [7168/24098 (30%)] Loss: 0.002262
Train Epoch: 4 [7680/24098 (32%)] Loss: 0.001030
Train Epoch: 4 [8192/24098 (34%)] Loss: 0.001289
Train Epoch: 4 [8704/24098 (36%)] Loss: 0.001527
Train Epoch: 4 [9216/24098 (38%)] Loss: 0.001348
Train Epoch: 4 [9728/24098 (40%)] Loss: 0.001516
Train Epoch: 4 [10240/24098 (42%)] Loss: 0.001206
Train Epoch: 4 [10752/24098 (45%)] Loss: 0.001421
Train Epoch: 4 [11264/24098 (47%)] Loss: 0.001435
Train Epoch: 4 [11776/24098 (49%)] Loss: 0.001534
Train Epoch: 4 [12288/24098 (51%)] Loss: 0.001243
Train Epoch: 4 [12800/24098 (53%)] Loss: 0.001588
Train Epoch: 4 [13312/24098 (55%)] Loss: 0.000878
Train Epoch: 4 [13824/24098 (57%)] Loss: 0.001494
Train Epoch: 4 [14336/24098 (59%)] Loss: 0.001153
Train Epoch: 4 [14848/24098 (62%)] Loss: 0.001153
Train Epoch: 4 [15360/24098 (64%)] Loss: 0.001073
Train Epoch: 4 [15872/24098 (66%)] Loss: 0.001375
Train Epoch: 4 [16384/24098 (68%)] Loss: 0.001343
Train Epoch: 4 [16896/24098 (70%)] Loss: 0.001479
Train Epoch: 4 [17408/24098 (72%)] Loss: 0.001129
Train Epoch: 4 [17920/24098 (74%)] Loss: 0.002011
Train Epoch: 4 [18432/24098 (76%)] Loss: 0.000973
Train Epoch: 4 [18944/24098 (79%)] Loss: 0.001266
Train Epoch: 4 [19456/24098 (81%)] Loss: 0.002059
Train Epoch: 4 [19968/24098 (83%)] Loss: 0.001294
Train Epoch: 4 [20480/24098 (85%)] Loss: 0.001533
Train Epoch: 4 [20992/24098 (87%)] Loss: 0.001146
Train Epoch: 4 [21504/24098 (89%)] Loss: 0.001160
Train Epoch: 4 [22016/24098 (91%)] Loss: 0.001233
Train Epoch: 4 [22528/24098 (93%)] Loss: 0.001300
Train Epoch: 4 [23040/24098 (96%)] Loss: 0.001223
Train Epoch: 4 [23552/24098 (98%)] Loss: 0.002376
Train Epoch: 4 [24064/24098 (100%)] Loss: 0.000738
[t2v_metrics]MovieClips epoch 4, R@1: 7.8, R@5: 24.7, R@10 34.3, R@50 61.5MedR: 26, MeanR: 126.0
[v2t_metrics]MovieClips epoch 4, R@1: 11.7, R@5: 30.0, R@10 38.8, R@50 65.3MedR: 21, MeanR: 110.9
    epoch          : 4
    loss           : 0.0014745910038717032
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.013547339476644993
    val_t2v_metrics_R1: 7.767857142857143
    val_t2v_metrics_R5: 24.702380952380953
    val_t2v_metrics_R10: 34.31547619047619
    val_t2v_metrics_R50: 61.48809523809524
    val_t2v_metrics_MedR: 26.0
    val_t2v_metrics_MeanR: 125.9889880952381
    val_t2v_metrics_geometric_mean_R1-R5-R10: 18.743183553249327
    val_v2t_metrics_R1: 11.726190476190476
    val_v2t_metrics_R5: 30.029761904761905
    val_v2t_metrics_R10: 38.75
    val_v2t_metrics_R50: 65.26785714285714
    val_v2t_metrics_MedR: 21.0
    val_v2t_metrics_MeanR: 110.92351190476191
    val_v2t_metrics_geometric_mean_R1-R5-R10: 23.89609007830162
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch4.pth ...
Saving current best: model_best.pth ...
Train Epoch: 5 [0/24098 (0%)] Loss: 0.000873
Train Epoch: 5 [512/24098 (2%)] Loss: 0.000655
Train Epoch: 5 [1024/24098 (4%)] Loss: 0.000716
Train Epoch: 5 [1536/24098 (6%)] Loss: 0.000790
Train Epoch: 5 [2048/24098 (8%)] Loss: 0.000529
Train Epoch: 5 [2560/24098 (11%)] Loss: 0.001074
Train Epoch: 5 [3072/24098 (13%)] Loss: 0.000833
Train Epoch: 5 [3584/24098 (15%)] Loss: 0.000362
Train Epoch: 5 [4096/24098 (17%)] Loss: 0.000516
Train Epoch: 5 [4608/24098 (19%)] Loss: 0.000444
Train Epoch: 5 [5120/24098 (21%)] Loss: 0.001149
Train Epoch: 5 [5632/24098 (23%)] Loss: 0.000718
Train Epoch: 5 [6144/24098 (25%)] Loss: 0.000530
Train Epoch: 5 [6656/24098 (28%)] Loss: 0.000340
Train Epoch: 5 [7168/24098 (30%)] Loss: 0.000479
Train Epoch: 5 [7680/24098 (32%)] Loss: 0.000479
Train Epoch: 5 [8192/24098 (34%)] Loss: 0.000722
Train Epoch: 5 [8704/24098 (36%)] Loss: 0.000575
Train Epoch: 5 [9216/24098 (38%)] Loss: 0.000649
Train Epoch: 5 [9728/24098 (40%)] Loss: 0.000624
Train Epoch: 5 [10240/24098 (42%)] Loss: 0.000971
Train Epoch: 5 [10752/24098 (45%)] Loss: 0.000643
Train Epoch: 5 [11264/24098 (47%)] Loss: 0.000923
Train Epoch: 5 [11776/24098 (49%)] Loss: 0.000806
Train Epoch: 5 [12288/24098 (51%)] Loss: 0.000844
Train Epoch: 5 [12800/24098 (53%)] Loss: 0.000736
Train Epoch: 5 [13312/24098 (55%)] Loss: 0.000634
Train Epoch: 5 [13824/24098 (57%)] Loss: 0.000732
Train Epoch: 5 [14336/24098 (59%)] Loss: 0.000559
Train Epoch: 5 [14848/24098 (62%)] Loss: 0.000585
Train Epoch: 5 [15360/24098 (64%)] Loss: 0.000638
Train Epoch: 5 [15872/24098 (66%)] Loss: 0.001115
Train Epoch: 5 [16384/24098 (68%)] Loss: 0.000588
Train Epoch: 5 [16896/24098 (70%)] Loss: 0.000435
Train Epoch: 5 [17408/24098 (72%)] Loss: 0.000612
Train Epoch: 5 [17920/24098 (74%)] Loss: 0.000647
Train Epoch: 5 [18432/24098 (76%)] Loss: 0.000532
Train Epoch: 5 [18944/24098 (79%)] Loss: 0.000524
Train Epoch: 5 [19456/24098 (81%)] Loss: 0.000476
Train Epoch: 5 [19968/24098 (83%)] Loss: 0.000527
Train Epoch: 5 [20480/24098 (85%)] Loss: 0.000705
Train Epoch: 5 [20992/24098 (87%)] Loss: 0.000856
Train Epoch: 5 [21504/24098 (89%)] Loss: 0.000633
Train Epoch: 5 [22016/24098 (91%)] Loss: 0.000636
Train Epoch: 5 [22528/24098 (93%)] Loss: 0.000884
Train Epoch: 5 [23040/24098 (96%)] Loss: 0.000462
Train Epoch: 5 [23552/24098 (98%)] Loss: 0.001015
Train Epoch: 5 [24064/24098 (100%)] Loss: 0.000503
[t2v_metrics]MovieClips epoch 5, R@1: 9.7, R@5: 26.5, R@10 37.1, R@50 64.5MedR: 22, MeanR: 119.6
[v2t_metrics]MovieClips epoch 5, R@1: 12.8, R@5: 31.6, R@10 40.7, R@50 67.2MedR: 18, MeanR: 108.4
    epoch          : 5
    loss           : 0.000603722138913961
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.013402835465967655
    val_t2v_metrics_R1: 9.732142857142858
    val_t2v_metrics_R5: 26.547619047619047
    val_t2v_metrics_R10: 37.05357142857143
    val_t2v_metrics_R50: 64.52380952380952
    val_t2v_metrics_MedR: 22.0
    val_t2v_metrics_MeanR: 119.64166666666667
    val_t2v_metrics_geometric_mean_R1-R5-R10: 21.23348968016972
    val_v2t_metrics_R1: 12.797619047619047
    val_v2t_metrics_R5: 31.577380952380953
    val_v2t_metrics_R10: 40.714285714285715
    val_v2t_metrics_R50: 67.23214285714286
    val_v2t_metrics_MedR: 18.0
    val_v2t_metrics_MeanR: 108.38511904761904
    val_v2t_metrics_geometric_mean_R1-R5-R10: 25.434158198114854
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch5.pth ...
Saving current best: model_best.pth ...
Train Epoch: 6 [0/24098 (0%)] Loss: 0.000367
Train Epoch: 6 [512/24098 (2%)] Loss: 0.000273
Train Epoch: 6 [1024/24098 (4%)] Loss: 0.000520
Train Epoch: 6 [1536/24098 (6%)] Loss: 0.000250
Train Epoch: 6 [2048/24098 (8%)] Loss: 0.000564
Train Epoch: 6 [2560/24098 (11%)] Loss: 0.000898
Train Epoch: 6 [3072/24098 (13%)] Loss: 0.000240
Train Epoch: 6 [3584/24098 (15%)] Loss: 0.000375
Train Epoch: 6 [4096/24098 (17%)] Loss: 0.000328
Train Epoch: 6 [4608/24098 (19%)] Loss: 0.000349
Train Epoch: 6 [5120/24098 (21%)] Loss: 0.000350
Train Epoch: 6 [5632/24098 (23%)] Loss: 0.000252
Train Epoch: 6 [6144/24098 (25%)] Loss: 0.000239
Train Epoch: 6 [6656/24098 (28%)] Loss: 0.000444
Train Epoch: 6 [7168/24098 (30%)] Loss: 0.000432
Train Epoch: 6 [7680/24098 (32%)] Loss: 0.000282
Train Epoch: 6 [8192/24098 (34%)] Loss: 0.000196
Train Epoch: 6 [8704/24098 (36%)] Loss: 0.000303
Train Epoch: 6 [9216/24098 (38%)] Loss: 0.000326
Train Epoch: 6 [9728/24098 (40%)] Loss: 0.000369
Train Epoch: 6 [10240/24098 (42%)] Loss: 0.000260
Train Epoch: 6 [10752/24098 (45%)] Loss: 0.000391
Train Epoch: 6 [11264/24098 (47%)] Loss: 0.000507
Train Epoch: 6 [11776/24098 (49%)] Loss: 0.000301
Train Epoch: 6 [12288/24098 (51%)] Loss: 0.000169
Train Epoch: 6 [12800/24098 (53%)] Loss: 0.000387
Train Epoch: 6 [13312/24098 (55%)] Loss: 0.000382
Train Epoch: 6 [13824/24098 (57%)] Loss: 0.000573
Train Epoch: 6 [14336/24098 (59%)] Loss: 0.000494
Train Epoch: 6 [14848/24098 (62%)] Loss: 0.000430
Train Epoch: 6 [15360/24098 (64%)] Loss: 0.000338
Train Epoch: 6 [15872/24098 (66%)] Loss: 0.000474
Train Epoch: 6 [16384/24098 (68%)] Loss: 0.000296
Train Epoch: 6 [16896/24098 (70%)] Loss: 0.000465
Train Epoch: 6 [17408/24098 (72%)] Loss: 0.000484
Train Epoch: 6 [17920/24098 (74%)] Loss: 0.000330
Train Epoch: 6 [18432/24098 (76%)] Loss: 0.000424
Train Epoch: 6 [18944/24098 (79%)] Loss: 0.000249
Train Epoch: 6 [19456/24098 (81%)] Loss: 0.000331
Train Epoch: 6 [19968/24098 (83%)] Loss: 0.000253
Train Epoch: 6 [20480/24098 (85%)] Loss: 0.000449
Train Epoch: 6 [20992/24098 (87%)] Loss: 0.000427
Train Epoch: 6 [21504/24098 (89%)] Loss: 0.000345
Train Epoch: 6 [22016/24098 (91%)] Loss: 0.000328
Train Epoch: 6 [22528/24098 (93%)] Loss: 0.000362
Train Epoch: 6 [23040/24098 (96%)] Loss: 0.000263
Train Epoch: 6 [23552/24098 (98%)] Loss: 0.000244
Train Epoch: 6 [24064/24098 (100%)] Loss: 0.000233
[t2v_metrics]MovieClips epoch 6, R@1: 11.0, R@5: 28.7, R@10 38.7, R@50 65.7MedR: 20, MeanR: 116.6
[v2t_metrics]MovieClips epoch 6, R@1: 13.4, R@5: 32.6, R@10 41.7, R@50 67.2MedR: 18, MeanR: 107.9
    epoch          : 6
    loss           : 0.00034007465694052113
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.013627481646835804
    val_t2v_metrics_R1: 10.952380952380953
    val_t2v_metrics_R5: 28.720238095238095
    val_t2v_metrics_R10: 38.660714285714285
    val_t2v_metrics_R50: 65.6547619047619
    val_t2v_metrics_MedR: 20.0
    val_t2v_metrics_MeanR: 116.58452380952382
    val_t2v_metrics_geometric_mean_R1-R5-R10: 22.996168576535865
    val_v2t_metrics_R1: 13.422619047619047
    val_v2t_metrics_R5: 32.648809523809526
    val_v2t_metrics_R10: 41.69642857142857
    val_v2t_metrics_R50: 67.23214285714286
    val_v2t_metrics_MedR: 18.0
    val_v2t_metrics_MeanR: 107.9297619047619
    val_v2t_metrics_geometric_mean_R1-R5-R10: 26.339113728891324
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch6.pth ...
Train Epoch: 7 [0/24098 (0%)] Loss: 0.000262
Train Epoch: 7 [512/24098 (2%)] Loss: 0.000183
Train Epoch: 7 [1024/24098 (4%)] Loss: 0.000248
Train Epoch: 7 [1536/24098 (6%)] Loss: 0.000274
Train Epoch: 7 [2048/24098 (8%)] Loss: 0.000229
Train Epoch: 7 [2560/24098 (11%)] Loss: 0.000227
Train Epoch: 7 [3072/24098 (13%)] Loss: 0.000271
Train Epoch: 7 [3584/24098 (15%)] Loss: 0.000310
Train Epoch: 7 [4096/24098 (17%)] Loss: 0.000120
Train Epoch: 7 [4608/24098 (19%)] Loss: 0.000258
Train Epoch: 7 [5120/24098 (21%)] Loss: 0.000140
Train Epoch: 7 [5632/24098 (23%)] Loss: 0.000221
Train Epoch: 7 [6144/24098 (25%)] Loss: 0.000180
Train Epoch: 7 [6656/24098 (28%)] Loss: 0.000259
Train Epoch: 7 [7168/24098 (30%)] Loss: 0.000170
Train Epoch: 7 [7680/24098 (32%)] Loss: 0.000251
Train Epoch: 7 [8192/24098 (34%)] Loss: 0.000160
Train Epoch: 7 [8704/24098 (36%)] Loss: 0.000259
Train Epoch: 7 [9216/24098 (38%)] Loss: 0.000138
Train Epoch: 7 [9728/24098 (40%)] Loss: 0.000210
Train Epoch: 7 [10240/24098 (42%)] Loss: 0.000252
Train Epoch: 7 [10752/24098 (45%)] Loss: 0.000254
Train Epoch: 7 [11264/24098 (47%)] Loss: 0.000147
Train Epoch: 7 [11776/24098 (49%)] Loss: 0.000203
Train Epoch: 7 [12288/24098 (51%)] Loss: 0.000192
Train Epoch: 7 [12800/24098 (53%)] Loss: 0.000137
Train Epoch: 7 [13312/24098 (55%)] Loss: 0.000313
Train Epoch: 7 [13824/24098 (57%)] Loss: 0.000131
Train Epoch: 7 [14336/24098 (59%)] Loss: 0.000521
Train Epoch: 7 [14848/24098 (62%)] Loss: 0.000195
Train Epoch: 7 [15360/24098 (64%)] Loss: 0.000186
Train Epoch: 7 [15872/24098 (66%)] Loss: 0.000229
Train Epoch: 7 [16384/24098 (68%)] Loss: 0.000173
Train Epoch: 7 [16896/24098 (70%)] Loss: 0.000266
Train Epoch: 7 [17408/24098 (72%)] Loss: 0.000154
Train Epoch: 7 [17920/24098 (74%)] Loss: 0.000154
Train Epoch: 7 [18432/24098 (76%)] Loss: 0.000212
Train Epoch: 7 [18944/24098 (79%)] Loss: 0.000100
Train Epoch: 7 [19456/24098 (81%)] Loss: 0.000190
Train Epoch: 7 [19968/24098 (83%)] Loss: 0.000316
Train Epoch: 7 [20480/24098 (85%)] Loss: 0.000185
Train Epoch: 7 [20992/24098 (87%)] Loss: 0.000228
Train Epoch: 7 [21504/24098 (89%)] Loss: 0.000145
Train Epoch: 7 [22016/24098 (91%)] Loss: 0.000189
Train Epoch: 7 [22528/24098 (93%)] Loss: 0.000276
Train Epoch: 7 [23040/24098 (96%)] Loss: 0.000405
Train Epoch: 7 [23552/24098 (98%)] Loss: 0.000212
Train Epoch: 7 [24064/24098 (100%)] Loss: 0.000091
[t2v_metrics]MovieClips epoch 7, R@1: 10.6, R@5: 29.1, R@10 39.1, R@50 65.5MedR: 20, MeanR: 116.9
[v2t_metrics]MovieClips epoch 7, R@1: 14.1, R@5: 33.1, R@10 42.9, R@50 67.8MedR: 17, MeanR: 106.3
    epoch          : 7
    loss           : 0.00022238692851886597
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.013892431743443012
    val_t2v_metrics_R1: 10.625
    val_t2v_metrics_R5: 29.077380952380953
    val_t2v_metrics_R10: 39.107142857142854
    val_t2v_metrics_R50: 65.53571428571429
    val_t2v_metrics_MedR: 20.0
    val_t2v_metrics_MeanR: 116.93095238095238
    val_t2v_metrics_geometric_mean_R1-R5-R10: 22.94634070342675
    val_v2t_metrics_R1: 14.107142857142858
    val_v2t_metrics_R5: 33.06547619047619
    val_v2t_metrics_R10: 42.857142857142854
    val_v2t_metrics_R50: 67.82738095238095
    val_v2t_metrics_MedR: 17.0
    val_v2t_metrics_MeanR: 106.29925595238095
    val_v2t_metrics_geometric_mean_R1-R5-R10: 27.140156867636914
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch7.pth ...
Train Epoch: 8 [0/24098 (0%)] Loss: 0.000206
Train Epoch: 8 [512/24098 (2%)] Loss: 0.000211
Train Epoch: 8 [1024/24098 (4%)] Loss: 0.000159
Train Epoch: 8 [1536/24098 (6%)] Loss: 0.000188
Train Epoch: 8 [2048/24098 (8%)] Loss: 0.000111
Train Epoch: 8 [2560/24098 (11%)] Loss: 0.000164
Train Epoch: 8 [3072/24098 (13%)] Loss: 0.000072
Train Epoch: 8 [3584/24098 (15%)] Loss: 0.000159
Train Epoch: 8 [4096/24098 (17%)] Loss: 0.000174
Train Epoch: 8 [4608/24098 (19%)] Loss: 0.000219
Train Epoch: 8 [5120/24098 (21%)] Loss: 0.000090
Train Epoch: 8 [5632/24098 (23%)] Loss: 0.000199
Train Epoch: 8 [6144/24098 (25%)] Loss: 0.000130
Train Epoch: 8 [6656/24098 (28%)] Loss: 0.000159
Train Epoch: 8 [7168/24098 (30%)] Loss: 0.000142
Train Epoch: 8 [7680/24098 (32%)] Loss: 0.000099
Train Epoch: 8 [8192/24098 (34%)] Loss: 0.000279
Train Epoch: 8 [8704/24098 (36%)] Loss: 0.000182
Train Epoch: 8 [9216/24098 (38%)] Loss: 0.000191
Train Epoch: 8 [9728/24098 (40%)] Loss: 0.000220
Train Epoch: 8 [10240/24098 (42%)] Loss: 0.000112
Train Epoch: 8 [10752/24098 (45%)] Loss: 0.000227
Train Epoch: 8 [11264/24098 (47%)] Loss: 0.000197
Train Epoch: 8 [11776/24098 (49%)] Loss: 0.000148
Train Epoch: 8 [12288/24098 (51%)] Loss: 0.000213
Train Epoch: 8 [12800/24098 (53%)] Loss: 0.000180
Train Epoch: 8 [13312/24098 (55%)] Loss: 0.000278
Train Epoch: 8 [13824/24098 (57%)] Loss: 0.000166
Train Epoch: 8 [14336/24098 (59%)] Loss: 0.000108
Train Epoch: 8 [14848/24098 (62%)] Loss: 0.000158
Train Epoch: 8 [15360/24098 (64%)] Loss: 0.000152
Train Epoch: 8 [15872/24098 (66%)] Loss: 0.000140
Train Epoch: 8 [16384/24098 (68%)] Loss: 0.000065
Train Epoch: 8 [16896/24098 (70%)] Loss: 0.000095
Train Epoch: 8 [17408/24098 (72%)] Loss: 0.000159
Train Epoch: 8 [17920/24098 (74%)] Loss: 0.000208
Train Epoch: 8 [18432/24098 (76%)] Loss: 0.000081
Train Epoch: 8 [18944/24098 (79%)] Loss: 0.000139
Train Epoch: 8 [19456/24098 (81%)] Loss: 0.000212
Train Epoch: 8 [19968/24098 (83%)] Loss: 0.000083
Train Epoch: 8 [20480/24098 (85%)] Loss: 0.000170
Train Epoch: 8 [20992/24098 (87%)] Loss: 0.000409
Train Epoch: 8 [21504/24098 (89%)] Loss: 0.000165
Train Epoch: 8 [22016/24098 (91%)] Loss: 0.000145
Train Epoch: 8 [22528/24098 (93%)] Loss: 0.000158
Train Epoch: 8 [23040/24098 (96%)] Loss: 0.000226
Train Epoch: 8 [23552/24098 (98%)] Loss: 0.000091
Train Epoch: 8 [24064/24098 (100%)] Loss: 0.000100
[t2v_metrics]MovieClips epoch 8, R@1: 11.3, R@5: 29.6, R@10 40.7, R@50 66.4MedR: 18, MeanR: 118.1
[v2t_metrics]MovieClips epoch 8, R@1: 14.2, R@5: 33.4, R@10 42.9, R@50 67.7MedR: 17, MeanR: 108.5
    epoch          : 8
    loss           : 0.00017877031002922134
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.014439526945352554
    val_t2v_metrics_R1: 11.30952380952381
    val_t2v_metrics_R5: 29.583333333333332
    val_t2v_metrics_R10: 40.714285714285715
    val_t2v_metrics_R50: 66.39880952380952
    val_t2v_metrics_MedR: 18.0
    val_t2v_metrics_MeanR: 118.07767857142858
    val_t2v_metrics_geometric_mean_R1-R5-R10: 23.88247959771395
    val_v2t_metrics_R1: 14.196428571428571
    val_v2t_metrics_R5: 33.36309523809524
    val_v2t_metrics_R10: 42.94642857142857
    val_v2t_metrics_R50: 67.73809523809524
    val_v2t_metrics_MedR: 17.0
    val_v2t_metrics_MeanR: 108.5389880952381
    val_v2t_metrics_geometric_mean_R1-R5-R10: 27.297581010989617
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch8.pth ...
Train Epoch: 9 [0/24098 (0%)] Loss: 0.000091
Train Epoch: 9 [512/24098 (2%)] Loss: 0.000106
Train Epoch: 9 [1024/24098 (4%)] Loss: 0.000301
Train Epoch: 9 [1536/24098 (6%)] Loss: 0.000097
Train Epoch: 9 [2048/24098 (8%)] Loss: 0.000079
Train Epoch: 9 [2560/24098 (11%)] Loss: 0.000168
Train Epoch: 9 [3072/24098 (13%)] Loss: 0.000150
Train Epoch: 9 [3584/24098 (15%)] Loss: 0.000075
Train Epoch: 9 [4096/24098 (17%)] Loss: 0.000073
Train Epoch: 9 [4608/24098 (19%)] Loss: 0.000157
Train Epoch: 9 [5120/24098 (21%)] Loss: 0.000119
Train Epoch: 9 [5632/24098 (23%)] Loss: 0.000146
Train Epoch: 9 [6144/24098 (25%)] Loss: 0.000114
Train Epoch: 9 [6656/24098 (28%)] Loss: 0.000122
Train Epoch: 9 [7168/24098 (30%)] Loss: 0.000120
Train Epoch: 9 [7680/24098 (32%)] Loss: 0.000092
Train Epoch: 9 [8192/24098 (34%)] Loss: 0.000207
Train Epoch: 9 [8704/24098 (36%)] Loss: 0.000110
Train Epoch: 9 [9216/24098 (38%)] Loss: 0.000077
Train Epoch: 9 [9728/24098 (40%)] Loss: 0.000170
Train Epoch: 9 [10240/24098 (42%)] Loss: 0.000089
Train Epoch: 9 [10752/24098 (45%)] Loss: 0.000156
Train Epoch: 9 [11264/24098 (47%)] Loss: 0.000087
Train Epoch: 9 [11776/24098 (49%)] Loss: 0.000145
Train Epoch: 9 [12288/24098 (51%)] Loss: 0.000080
Train Epoch: 9 [12800/24098 (53%)] Loss: 0.000097
Train Epoch: 9 [13312/24098 (55%)] Loss: 0.000103
Train Epoch: 9 [13824/24098 (57%)] Loss: 0.000089
Train Epoch: 9 [14336/24098 (59%)] Loss: 0.000064
Train Epoch: 9 [14848/24098 (62%)] Loss: 0.000107
Train Epoch: 9 [15360/24098 (64%)] Loss: 0.000056
Train Epoch: 9 [15872/24098 (66%)] Loss: 0.000092
Train Epoch: 9 [16384/24098 (68%)] Loss: 0.000129
Train Epoch: 9 [16896/24098 (70%)] Loss: 0.000120
Train Epoch: 9 [17408/24098 (72%)] Loss: 0.000178
Train Epoch: 9 [17920/24098 (74%)] Loss: 0.000097
Train Epoch: 9 [18432/24098 (76%)] Loss: 0.000199
Train Epoch: 9 [18944/24098 (79%)] Loss: 0.000136
Train Epoch: 9 [19456/24098 (81%)] Loss: 0.000031
Train Epoch: 9 [19968/24098 (83%)] Loss: 0.000044
Train Epoch: 9 [20480/24098 (85%)] Loss: 0.000106
Train Epoch: 9 [20992/24098 (87%)] Loss: 0.000085
Train Epoch: 9 [21504/24098 (89%)] Loss: 0.000219
Train Epoch: 9 [22016/24098 (91%)] Loss: 0.000058
Train Epoch: 9 [22528/24098 (93%)] Loss: 0.000110
Train Epoch: 9 [23040/24098 (96%)] Loss: 0.000109
Train Epoch: 9 [23552/24098 (98%)] Loss: 0.000122
Train Epoch: 9 [24064/24098 (100%)] Loss: 0.000054
[t2v_metrics]MovieClips epoch 9, R@1: 11.6, R@5: 30.4, R@10 40.7, R@50 67.4MedR: 18, MeanR: 117.2
[v2t_metrics]MovieClips epoch 9, R@1: 14.9, R@5: 33.6, R@10 44.0, R@50 68.6MedR: 16, MeanR: 106.8
    epoch          : 9
    loss           : 0.00013642922812774505
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.014479894191026688
    val_t2v_metrics_R1: 11.607142857142858
    val_t2v_metrics_R5: 30.386904761904763
    val_t2v_metrics_R10: 40.714285714285715
    val_t2v_metrics_R50: 67.35119047619048
    val_t2v_metrics_MedR: 18.0
    val_t2v_metrics_MeanR: 117.2110119047619
    val_t2v_metrics_geometric_mean_R1-R5-R10: 24.30633829502482
    val_v2t_metrics_R1: 14.94047619047619
    val_v2t_metrics_R5: 33.63095238095238
    val_v2t_metrics_R10: 44.017857142857146
    val_v2t_metrics_R50: 68.63095238095238
    val_v2t_metrics_MedR: 16.0
    val_v2t_metrics_MeanR: 106.78303571428572
    val_v2t_metrics_geometric_mean_R1-R5-R10: 28.070113422491637
Saving checkpoint: saved/models/MoEE/0318_074506/checkpoint-epoch9.pth ...
Train Epoch: 10 [0/24098 (0%)] Loss: 0.000085
Train Epoch: 10 [512/24098 (2%)] Loss: 0.000092
Train Epoch: 10 [1024/24098 (4%)] Loss: 0.000043
Train Epoch: 10 [1536/24098 (6%)] Loss: 0.000123
Train Epoch: 10 [2048/24098 (8%)] Loss: 0.000133
Train Epoch: 10 [2560/24098 (11%)] Loss: 0.000037
Train Epoch: 10 [3072/24098 (13%)] Loss: 0.000081
Train Epoch: 10 [3584/24098 (15%)] Loss: 0.000074
Train Epoch: 10 [4096/24098 (17%)] Loss: 0.000048
Train Epoch: 10 [4608/24098 (19%)] Loss: 0.000095
Train Epoch: 10 [5120/24098 (21%)] Loss: 0.000084
Train Epoch: 10 [5632/24098 (23%)] Loss: 0.000457
Train Epoch: 10 [6144/24098 (25%)] Loss: 0.000126
Train Epoch: 10 [6656/24098 (28%)] Loss: 0.000100
Train Epoch: 10 [7168/24098 (30%)] Loss: 0.000050
Train Epoch: 10 [7680/24098 (32%)] Loss: 0.000071
Train Epoch: 10 [8192/24098 (34%)] Loss: 0.000071
Train Epoch: 10 [8704/24098 (36%)] Loss: 0.000116
Train Epoch: 10 [9216/24098 (38%)] Loss: 0.000155
Train Epoch: 10 [9728/24098 (40%)] Loss: 0.000185
Train Epoch: 10 [10240/24098 (42%)] Loss: 0.000040
Train Epoch: 10 [10752/24098 (45%)] Loss: 0.000169
Train Epoch: 10 [11264/24098 (47%)] Loss: 0.000075
Train Epoch: 10 [11776/24098 (49%)] Loss: 0.000133
Train Epoch: 10 [12288/24098 (51%)] Loss: 0.000067
Train Epoch: 10 [12800/24098 (53%)] Loss: 0.000111
Train Epoch: 10 [13312/24098 (55%)] Loss: 0.000047
Train Epoch: 10 [13824/24098 (57%)] Loss: 0.000072
Train Epoch: 10 [14336/24098 (59%)] Loss: 0.000220
Train Epoch: 10 [14848/24098 (62%)] Loss: 0.000045
Train Epoch: 10 [15360/24098 (64%)] Loss: 0.000112
Train Epoch: 10 [15872/24098 (66%)] Loss: 0.000092
Train Epoch: 10 [16384/24098 (68%)] Loss: 0.000056
Train Epoch: 10 [16896/24098 (70%)] Loss: 0.000113
Train Epoch: 10 [17408/24098 (72%)] Loss: 0.000121
Train Epoch: 10 [17920/24098 (74%)] Loss: 0.000112
Train Epoch: 10 [18432/24098 (76%)] Loss: 0.000059
Train Epoch: 10 [18944/24098 (79%)] Loss: 0.000098
Train Epoch: 10 [19456/24098 (81%)] Loss: 0.000172
Train Epoch: 10 [19968/24098 (83%)] Loss: 0.000050
Train Epoch: 10 [20480/24098 (85%)] Loss: 0.000102
Train Epoch: 10 [20992/24098 (87%)] Loss: 0.000077
Train Epoch: 10 [21504/24098 (89%)] Loss: 0.000077
Train Epoch: 10 [22016/24098 (91%)] Loss: 0.000261
Train Epoch: 10 [22528/24098 (93%)] Loss: 0.000091
Train Epoch: 10 [23040/24098 (96%)] Loss: 0.000172
Train Epoch: 10 [23552/24098 (98%)] Loss: 0.000119
Train Epoch: 10 [24064/24098 (100%)] Loss: 0.000091
[t2v_metrics]MovieClips epoch 10, R@1: 11.5, R@5: 30.3, R@10 40.3, R@50 65.7MedR: 19, MeanR: 120.5
[v2t_metrics]MovieClips epoch 10, R@1: 14.8, R@5: 33.5, R@10 43.2, R@50 67.9MedR: 16.5, MeanR: 109.9
    epoch          : 10
    loss           : 0.0001086858772671246
    t2v_metrics    : 0.0
    v2t_metrics    : 0.0
    val_loss       : 0.0150912469252944
    val_t2v_metrics_R1: 11.458333333333334
    val_t2v_metrics_R5: 30.327380952380953
    val_t2v_metrics_R10: 40.29761904761905
    val_t2v_metrics_R50: 65.6547619047619
    val_t2v_metrics_MedR: 19.0
    val_t2v_metrics_MeanR: 120.52202380952382
    val_t2v_metrics_geometric_mean_R1-R5-R10: 24.103415200596697
    val_v2t_metrics_R1: 14.791666666666666
    val_v2t_metrics_R5: 33.45238095238095
    val_v2t_metrics_R10: 43.24404761904762
    val_v2t_metrics_R50: 67.88690476190476
    val_v2t_metrics_MedR: 16.5
    val_v2t_metrics_MeanR: 109.89910714285715
    val_v2t_metrics_geometric_mean_R1-R5-R10: 27.762388501872795
Validation performance didn't improve for 4 epochs. Training stops.